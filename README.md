# ml-refresher
A ChatGPT-4 generated ml exercise refresher
### Summary 
I asked chat gpt to generate a game plan for learning how to implement an LLM from scratch given my exp. in CS
After some tuning here are the topic results. I also asked it to generate fill in the blank jupyter notebooks 
for the key functionality to speed things up.

### Topics
Given your background in computer science and development, you already possess a solid foundation for understanding machine learning concepts. However, it's generally better to build knowledge sequentially, as it helps to grasp more advanced concepts more easily. While you could start directly with building a simple LLM from scratch, you may miss some essential background knowledge that would make the process smoother and more comprehensive.

Here's a recommended learning sequence:

- Machine Learning Basics: Start by learning supervised and unsupervised learning, classification, regression, and clustering algorithms. This will help you understand the fundamentals of training models, loss functions, and optimization techniques.

- Deep Learning Concepts: Learn about neural networks, activation functions, backpropagation, convolutional neural networks (CNNs), and recurrent neural networks (RNNs). This will provide you with the necessary background to understand the more advanced architectures used in LLMs.

- Natural Language Processing (NLP): Study NLP techniques like tokenization, stemming, part-of-speech tagging, and word embeddings. This will help you understand how text data is processed and how models learn to represent language.

- LLM Architectures and Transformer Models: After gaining a solid understanding of the previous steps, you can dive into LLMs, transformers, and self-attention mechanisms. This will enable you to understand the specific techniques used in LLMs and their differences from other neural network architectures.

- Implement a Simple LLM: With a strong foundation in the previous topics, you can now build a small-scale LLM from scratch. This hands-on experience will solidify your understanding of LLMs and their inner workings.

By following this learning sequence, you'll develop a comprehensive understanding of the fundamental concepts involved in LLMs. This will make it easier for you to grasp more advanced topics and techniques in the field.

### Exercises

#### Machine Learning Basics:
- a. Implement linear regression from scratch and train it on a synthetic dataset.
- b. Build a logistic regression model for binary classification using gradient descent optimization.
- c. Implement a k-means clustering algorithm and apply it to a 2D dataset for visualization.

#### Deep Learning Concepts:
- a. Implement a simple feedforward neural network (multi-layer perceptron) for classifying the MNIST dataset.
- b. Build a convolutional neural network (CNN) to classify the CIFAR-10 dataset.
- c. Create a basic recurrent neural network (RNN) or LSTM for sentiment analysis on movie reviews.

#### Natural Language Processing (NLP):
- a. Write Python code to tokenize, stem, and remove stop words from a given text corpus using NLTK.
- b. Implement the TF-IDF algorithm from scratch and apply it to a document classification task.
- c. Train a Word2Vec model on a text corpus and visualize the learned embeddings using t-SNE.

#### LLM Architectures and Transformer Models:
- a. Implement a self-attention mechanism in Python and apply it to a sequence-to-sequence task.
- b. Build a basic transformer model from scratch and train it on a small-scale translation task.
- c. Fine-tune a pre-trained BERT model for a sentiment analysis task using the Hugging Face Transformers library.

#### Implement a Simple LLM:
- a. Build a small-scale transformer-based LLM with a limited vocabulary and train it on a subset of the Penn Treebank dataset for language modeling.
- b. Implement a masked language model (similar to BERT) using your small-scale transformer and fine-tune it for a named entity recognition task.
- c. Explore the effect of different positional encoding techniques on your small-scale LLM's performance.

These exercises will help you deepen your understanding of the topics and gain practical experience in implementing machine learning algorithms and models. Be sure to pay attention to problem areas like dimensionality, overfitting, and computational complexity as you work through the exercises.
